회사에서 모니터링 업무를 맡으며 직접 elastalert를 깔고, metricbeat를 깔며 진행해본 결과, 다른 모듈도 많고 기존의  팀원 분이 이미 metricbeat와 heartbeat를 사용하고 계셔서 따로 내가 space에서 timestamp에 DTO를 설정할 필요도, query를 짤 필요도 없었다.

다만, 연결이 끊겼다가 다시 연결되는 불안정함이 있었고, hyperledger에서 문제가 생긴건지는 모르지만 alert가 제대로 slack으로 들어오지 않고 있었다.

그래서 이 모니터링을 위해 모든 서버 마다 metricbeat를 깔고 kibana와 연결을 해준 뒤 space를 만들어 elastic과 잘 연동 되는지 확인하는 작업을 반복하는 일을 하고 있었다.

나중에 서비스의 규모가 더 커진다면 서버도 증설하게 될텐데 그때 가서도 이렇게 비효율적으로 하나하나 깔아서 사람 한 명이 붙어서 진행하게 된다면 인력 낭비, 시간 낭비, 효율 낭비 아닌가…?

어떻게 해야 더 좋은 구조, 확장성 있는 모니터링 시스템을 만들 수 있을까 고민하다 kafka를 알게 되었다. 카카오에서도 사용하는 데이터 파이프라인이라고 한다. 기존의 linux 환경에서 바로 실행할 수 있다는 점이 좋았다. 

무엇보다 카프카는 빠르다. 

카프카는 기존의 메시징 시스템과 동일한 비동기 시스템! 매우 효율적이다.

[https://www.confluent.io/blog/event-streaming-platform-1/](https://www.confluent.io/blog/event-streaming-platform-1/)

기존의 kafka는 메시징 서버로 동작한다. 여기서 메시징 서버는 무엇인가?

pub/sub 모델 방식의 메시징 서버가 있다. 그리고 중간에 메시징 시스템을 추가한 pub/sub 모델 메시징 서버가 있다. 하지만 후자의 경우 성능 문제가 있어 잘 사용하지 못하고 있었다. 그래서 이런 문제를 해결하며 나온 것이 카프카이다.

메시지 교환 전달의 신뢰성 관리를 프로듀서와 컨슈머 쪽으로 넘기고, 부하가 많이 걸리는 교환기 기능 역시 컨슈머가 만들 수 있게 함으로써 메시징 시스템 내에서의 작업량을 줄이고 ㅇ이렇게 절약한 작업량을 메시징 전달 성능에 집중하여 고성능 메시징 시스템을 만든 것이다.

pub/sub 방식에는 성능 이슈가 있는데도 선택한 이유는 메시징 전송 방식 중 메시지를 보내는 역할과 받는 역할이 완벽히 분리되길 원했기 때문이다.

왜냐하면 메트릭 수집이 늦어지며 수집이나 처리 시간이 늦어지는 문제점이 발견되었기 때문 (현재 사내에서 사용하는 방식도 특정 시점에, 특정 조건을 만족하면 alert를 slack으로 발송하게끔 구성되어 있다.)

무엇보다 카프카는 확장성이 뛰어나다. 카프카를 통해 고성능의 펍/섭 모델이 가능해지며 서비스 기반 아키텍쳐(SOA, Service Oriented Architecture)의 핵심 구성 요소 중 하나인 엔터프라이즈 서비스 버스(ESB, Enterprise Service Bus)를 쉽게 구현할 수 있게 되었다.